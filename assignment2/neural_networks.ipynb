{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1) Prelimanary overview =  the two datasets contain involve analysis and effect of various objective which are treated as features/variables for modelling purposes on the quality of wine for two different classes of wine. The last column contains the assigned quality to the given wine sample based on various features which we can treat as target values for regression problem. Exact quantification can be found in attached documentaion which validates its use for scienctific purposes especially regression and feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 b) Explore = invloves identifying feautures , target values , whether their are any null points or not, identify outliers\n",
    "\n",
    "Visulaise = maybe print few rows (chat gpt recommendation), quantify no. of null points, outliers\n",
    "print some basic statistcs like mean median mode\n",
    "\n",
    "Preprocessing = fill null points using strategies like imputation(gpt), removing outliers\n",
    "chat gpt := which may include feature scaling, normalization, or encoding categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import types\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##explore and visualize\n",
    "\n",
    "# Load the dataset\n",
    "wine_red = pd.read_csv(\"winequality-red.csv\", sep=';')\n",
    "wine_white = pd.read_csv(\"winequality-white.csv\", sep=';')\n",
    "\n",
    "##deeisplay various parameters asosciated with data\n",
    "##convection input matrix = N x D\n",
    "\n",
    "##no. of rows = no of samples\n",
    "N_red = wine_red.shape[0] ##axis 0\n",
    "N_white = wine_white.shape[0] ##axis 0\n",
    "##no of features= no of columns - 1(target column)\n",
    "D_red = wine_red.shape[1] - 1 ##axis 1\n",
    "D_white = wine_white.shape[1] - 1 ##axis 1 \n",
    "\n",
    "##seperate features and target\n",
    "featuresRed = wine_red.drop('quality', axis=1)##quality = target selectds all columns expect quality\n",
    "targetRed = wine_red['quality']\n",
    "featuresWhite = wine_white.drop('quality', axis=1)\n",
    "targetWhite = wine_white['quality']\n",
    "\n",
    "##desriptive stats assosciated with data ref : https://www.w3schools.com/python/pandas/ref_df_describe.asp \n",
    "Des_Red = wine_red.describe() ##obj connecting description (mean , median mode, max etc of data)\n",
    "Des_white = wine_white.describe()\n",
    "\n",
    "##null points\n",
    "##ref chat gpt ,is_null converts every datapoint to bool (Non empty => 0 empty => non zero)\n",
    "NullCountRed = wine_red.isnull().sum() ##sum all non zero gives null count in every column\n",
    "NullCountWhite = wine_white.isnull().sum()\n",
    "\n",
    "##outliers bas\n",
    "##ref : chatgpt IQR method basically calculate data range within which 50% of middle data points\n",
    "def count_outliers(df, columnName):\n",
    "    Q1 = df[columnName].quantile(0.25) # first quartile of data\n",
    "    Q3 = df[columnName].quantile(0.75)# rnd quartile \n",
    "    IQR = Q3 - Q1  #3 range we are assuming to contain data (excluding outliers)\n",
    "    lower_bound = Q1 - 1.5 * IQR ##chat gpt selected appropriate limits below which we can easily say data is outlier\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "     ##chat gpt syntax\n",
    "     ##only selects that part of selected column of dataframe which lies in limits\n",
    "    df_removedOutliers = (df[columnName] >= lower_bound) & (df[columnName] <= upper_bound)\n",
    "    OutliersCount = df_removedOutliers.shape - df_removedOutliers.sum() ##only true i.e 1 counted in sum\n",
    "    return OutliersCount\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given data has 1599 samples\n",
      "  \n",
      "11  no. of features\n",
      " \n",
      "28 no of ouliers in target set\n",
      "  \n",
      "fixed acidity           0\n",
      "volatile acidity        0\n",
      "citric acid             0\n",
      "residual sugar          0\n",
      "chlorides               0\n",
      "free sulfur dioxide     0\n",
      "total sulfur dioxide    0\n",
      "density                 0\n",
      "pH                      0\n",
      "sulphates               0\n",
      "alcohol                 0\n",
      "quality                 0\n",
      "dtype: int64 no. of null points in each feature and target set\n",
      "\n",
      "description as follow        fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
      "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
      "mean        8.319637          0.527821     0.270976        2.538806   \n",
      "std         1.741096          0.179060     0.194801        1.409928   \n",
      "min         4.600000          0.120000     0.000000        0.900000   \n",
      "25%         7.100000          0.390000     0.090000        1.900000   \n",
      "50%         7.900000          0.520000     0.260000        2.200000   \n",
      "75%         9.200000          0.640000     0.420000        2.600000   \n",
      "max        15.900000          1.580000     1.000000       15.500000   \n",
      "\n",
      "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
      "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
      "mean      0.087467            15.874922             46.467792     0.996747   \n",
      "std       0.047065            10.460157             32.895324     0.001887   \n",
      "min       0.012000             1.000000              6.000000     0.990070   \n",
      "25%       0.070000             7.000000             22.000000     0.995600   \n",
      "50%       0.079000            14.000000             38.000000     0.996750   \n",
      "75%       0.090000            21.000000             62.000000     0.997835   \n",
      "max       0.611000            72.000000            289.000000     1.003690   \n",
      "\n",
      "                pH    sulphates      alcohol      quality  \n",
      "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
      "mean      3.311113     0.658149    10.422983     5.636023  \n",
      "std       0.154386     0.169507     1.065668     0.807569  \n",
      "min       2.740000     0.330000     8.400000     3.000000  \n",
      "25%       3.210000     0.550000     9.500000     5.000000  \n",
      "50%       3.310000     0.620000    10.200000     6.000000  \n",
      "75%       3.400000     0.730000    11.100000     6.000000  \n",
      "max       4.010000     2.000000    14.900000     8.000000  \n"
     ]
    }
   ],
   "source": [
    "##visulaize the exploration above for red data same will be for white\n",
    "outNumber = count_outliers(wine_red,\"quality\")\n",
    "\n",
    "print(f\"\"\"The given data has {N_red} samples\\n  \n",
    "{D_red}  no. of features\\n \n",
    "{outNumber[0]} no of ouliers in target set\\n  \n",
    "{NullCountRed} no. of null points in each feature and target set\\n\n",
    "description as follow {Des_Red}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pre processing\n",
    "\n",
    "##eliminate outliers\n",
    "def remove_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25) # first quartile of data\n",
    "    Q3 = df[column].quantile(0.75)# rnd quartile \n",
    "    IQR = Q3 - Q1  #3 range we are assuming to contain data (excluding outliers)\n",
    "    lower_bound = Q1 - 1.5 * IQR ##chat gpt selected appropriate limits below which we can easily say data is outlier\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    ##replace outliers with more appropriate values say median\n",
    "    median = df[column].median()\n",
    "    ##chat gpt syntax\n",
    "    ##basically uses lambda function to check if given data point is an outlier and if yes then replace with median\n",
    "    df[column] = df[column].apply(lambda x: median_value if x < lower_bound or x > upper_bound else x)\n",
    "    return df\n",
    "\n",
    "##fill null points\n",
    "##ref :  https://scikit-learn.org/stable/modules/impute.html from Q2 and chat gpt\n",
    "def impute_data(data):\n",
    "    ##select feautres to impute\n",
    "    features_for_imputation = data.columns.tolist()\n",
    "    # Initialize IterativeImputer\n",
    "    imputer = IterativeImputer(random_state=0)\n",
    "    # Fit and transform the data using IterativeImputer chat gpt \n",
    "    data_imputed = pd.DataFrame(imputer.fit_transform(data[features_for_imputation]), columns=features_for_imputation)\n",
    "    ##transformed data\n",
    "    data[features_for_imputation] = data_imputed\n",
    "\n",
    "\n",
    "##split data\n",
    "##ref : https://www.geeksforgeeks.org/how-to-split-the-dataset-with-scikit-learns-train_test_split-function/\n",
    "def trainTestSplit(X,y):\n",
    "    ##X = FEATURE AND Y =TARGET\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42) ##70 % for training\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    return [[X_train, X_test, X_val,y_train], [y_test,  y_val]]\n",
    "\n",
    "##standardize values\n",
    "def standardize(X_train, X_val, X_test):\n",
    "    # Standardize features to normalize independent features might be having different scales/units\n",
    "    #3 ref of code snippet : https://www.digitalocean.com/community/tutorials/standardscaler-function-in-python\n",
    "    scaler = StandardScaler()\n",
    "    ##scale training , testing and validation data \n",
    "    X_train_scaled = scaler.fit_transform(X_train) \n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled, X_val_scaled\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 b) lets make a class for stuff done yet for this part which we can use again again across questions\n",
    "Can be directly used in comprehensive python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessedData:\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        self.N = data.shape[0] ##axis 0\n",
    "        self.D = data.shape[1] - 1 ##axis 1 \n",
    "        self.features = data.drop('quality', axis=1)##quality = target selectds all columns expect quality\n",
    "        self.target = data['quality']\n",
    "        self.description = data.describe()\n",
    "        self.NullCount= data.isnull().sum()\n",
    "        \n",
    "    def count_outliers(self, columnName):\n",
    "        df = self.data\n",
    "        Q1 = df[columnName].quantile(0.25) # first quartile of data\n",
    "        Q3 = df[columnName].quantile(0.75)# rnd quartile \n",
    "        IQR = Q3 - Q1  #3 range we are assuming to contain data (excluding outliers)\n",
    "        lower_bound = Q1 - 1.5 * IQR ##chat gpt selected appropriate limits below which we can easily say data is outlier\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df_removedOutliers = (df[columnName] >= lower_bound) & (df[columnName] <= upper_bound)\n",
    "        self.OutliersCount = df_removedOutliers.shape - df_removedOutliers.sum() ##only true i.e 1 counted in sum\n",
    "        return OutliersCount\n",
    "    \n",
    "    def remove_outliers(self, column):\n",
    "        df = self.data\n",
    "        Q1 = df[column].quantile(0.25) # first quartile of data\n",
    "        Q3 = df[column].quantile(0.75)# rnd quartile \n",
    "        IQR = Q3 - Q1  #3 range we are assuming to contain data (excluding outliers)\n",
    "        lower_bound = Q1 - 1.5 * IQR ##chat gpt selected appropriate limits below which we can easily say data is outlier\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        median = df[column].median()\n",
    "        df[column] = df[column].apply(lambda x: median if x < lower_bound or x > upper_bound else x)\n",
    "        self.data = df\n",
    "        \n",
    "    def impute_data(self):\n",
    "        ##select feautres to impute\n",
    "        features_for_imputation = self.data.columns.tolist()\n",
    "        # Initialize IterativeImputer\n",
    "        imputer = IterativeImputer(random_state=0)\n",
    "        # Fit and transform the data using IterativeImputer ref : chat gpt \n",
    "        data_imputed = pd.DataFrame(imputer.fit_transform(self.data[features_for_imputation]), columns=features_for_imputation)\n",
    "        ##transformed data\n",
    "        self.data[features_for_imputation] = data_imputed\n",
    "\n",
    "    def trainTestSplit(self):\n",
    "        ##X = FEATURE AND Y =TARGET, of new proccedData which is stored in self.data\n",
    "        X = self.data.drop('quality', axis=1)\n",
    "        y = self.data[\"quality\"]\n",
    "        self.X_train, X_temp, self.y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42) ##70 % for training\n",
    "        self.X_val, self.X_test, self.y_val, self.y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    def standardize(self):\n",
    "        scaler = StandardScaler()\n",
    "        self.X_train_scaled = scaler.fit_transform(self.X_train) \n",
    "        self.X_val_scaled = scaler.transform(self.X_val)\n",
    "        self.X_test_scaled = scaler.transform(self.X_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 c) Train, validate varying at least one hyperparameter, and test at least two types of models:\n",
    "i. Random forest\n",
    "ii. Support vector regression with RBF kernel\n",
    "iii. Neural network with single hidden layer (output layer should have linear activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets make a class for each model which can be used repeatedely\n",
    "\n",
    "class model:\n",
    "    ##model for a given dataset\n",
    "    def __init__(self,ProcessedData): ##pass object of type ProcessedData (have all member functions,variables)\n",
    "        self.data = ProcessedData\n",
    "        ##Processed data must have been splited in testing and training set a priori\n",
    "        self.X_train_scaled = ProcessedData.X_train_scaled  ##we need training data for this class\n",
    "        self.y_train = ProcessedData.y_train\n",
    "    # 1. Random Forest  (creates multiple decesion tress and average out the prediction value of each tree)\n",
    "    ##ref chat gpt and  scikit_learn RandomForestRegressor\n",
    "    \n",
    "    def randomForest(self): \n",
    "        rf_model = RandomForestRegressor(random_state=42) ## initialize with initial random value 42(chat gpt) reproducibility\n",
    "        Paramgrid_rf = {'n_estimators': [100, 200, 300], 'max_depth': [None, 5, 10]}##hyperparameters varying = no of trees, depth of trees\n",
    "        ##use GridSearchCV to minimize mean square error , and find best hyperparams to fit model on training data\n",
    "        grid_rf = GridSearchCV(rf_model, Paramgrid_rf, cv=3, scoring='neg_mean_squared_error')## 3 fold verification\n",
    "        grid_rf.fit(self.X_train_scaled, self.y_train)##fit model\n",
    "        n_estimators, max_depth =  grid_rf.best_params_ #3optimum hyperparams\n",
    "        ##corresponding best model\n",
    "        Best_rf_model = grid_rf.best_estimator_\n",
    "        return Best_rf_model\n",
    "    \n",
    "    ##2. SVR using rbf kernel    \n",
    "    ##ref chat gpt and  scikit_learn SVR\n",
    "    def SVR(self):   \n",
    "        svr_model = SVR(kernel='rbf') ##initialize with rbf kernel\n",
    "        Paramgrid_svr = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10]}##hyperparams varying = regularization coefficient(prevents overfitting) and kernel coefficient for rbf\n",
    "        ##use GridSearchCV to minimize mean square error , and find best hyperparams to fit model on training data\n",
    "        grid_svr = GridSearchCV(svr_model, Paramgrid_svr, cv=3, scoring='neg_mean_squared_error')##search on grid to find best hyperparams\n",
    "        grid_svr.fit(self.X_train_scaled, self.y_train) ##3 fit modekl\n",
    "        ##optimum c and gamma\n",
    "        c, gamma =  grid_svr.best_params_\n",
    "        ##return best model\n",
    "        return grid_svr.best_estimator_\n",
    "    \n",
    "    ##neural networks\n",
    "    ## ref https://www.analyticsvidhya.com/blog/2021/11/training-neural-network-with-keras-and-basics-of-deep-learning/\n",
    "    ## simple feedforward neural network with 1 hidden layer\n",
    "    def CreateNeuralNetwork(self):\n",
    "        nn_model = Sequential() ##linear stack of layers in keras\n",
    "        ## 10 neurons per layer, relu activation functiom\n",
    "        nn_model.add(Dense(10, input_dim=self.X_train_scaled.shape[1], activation='relu'))\n",
    "        ##output layer with linear activation as asked in Ques\n",
    "        nn_model.add(Dense(1, activation='linear')) # linear activation\n",
    "        ##loss = mean sq error for regression, call adams optimizer for keras\n",
    "        nn_model.compile(optimizer='adam', loss='mean_squared_error') #minimize mean square error\n",
    "        return nn_model\n",
    "    \n",
    "    ##now using kerras generate a nn model we can use\n",
    "    def NN(self):\n",
    "        nn_model = KerasRegressor(build_fn=self.CreateNeuralNetwork, epochs=50, batch_size=32, verbose=0)\n",
    "        ##finally fir model with training data\n",
    "        nn_model.fit(self.X_train_scaled, self.y_train)\n",
    "        return nn_model\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For subsequent question from now on we can directly use these classes for processing data and deploying various models  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Validation MSE: 0.3111543055555555\n",
      "SVR Validation MSE: 0.3419715627591982\n",
      "SVR Validation MSE: 0.38032304744574225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8147/214265759.py:52: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  nn_model = KerasRegressor(build_fn=self.CreateNeuralNetwork, epochs=50, batch_size=32, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Validation MSE: 0.8972403624263711\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'model' object has no attribute 'nn_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeural Network Validation MSE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, nn_mseRed)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m##same for white\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m nn_modelWhite \u001b[38;5;241m=\u001b[39m \u001b[43mModelWhite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn_model\u001b[49m() \u001b[38;5;66;03m##initialized model\u001b[39;00m\n\u001b[1;32m     54\u001b[0m nn_predictionsWhite \u001b[38;5;241m=\u001b[39m nn_modelWhite\u001b[38;5;241m.\u001b[39mpredict(White\u001b[38;5;241m.\u001b[39mX_val_scaled) \u001b[38;5;66;03m##predict\u001b[39;00m\n\u001b[1;32m     55\u001b[0m nn_mseWhite \u001b[38;5;241m=\u001b[39m mean_squared_error(White\u001b[38;5;241m.\u001b[39my_val, nn_predictionsRed)\u001b[38;5;66;03m## cal error\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'model' object has no attribute 'nn_model'"
     ]
    }
   ],
   "source": [
    "## Q1 c using the classes we defined above\n",
    "# Load the dataset\n",
    "wine_red = pd.read_csv(\"winequality-red.csv\", sep=';')\n",
    "wine_white = pd.read_csv(\"winequality-white.csv\", sep=';')\n",
    "## create instance of class ProcessData\n",
    "Red = ProcessedData(wine_red)\n",
    "White = ProcessedData(wine_white)\n",
    "##remove outliers in target\n",
    "Red.remove_outliers(\"quality\")\n",
    "White.remove_outliers(\"quality\")\n",
    "##fill null points\n",
    "Red.impute_data()\n",
    "White.impute_data()\n",
    "##split \n",
    "Red.trainTestSplit()\n",
    "White.trainTestSplit()\n",
    "##finally standardise\n",
    "Red.standardize()\n",
    "White.standardize()\n",
    "\n",
    "##now our datasets have been processed and splitted into training test and validation \n",
    "##train the three models\n",
    "ModelRed = model(Red) ##instance of model class with different data\n",
    "ModelWhite = model(White)\n",
    "\n",
    "# Validate Random Forest model\n",
    "rfRed = ModelRed.randomForest()\n",
    "RfpredictionsRed = rfRed.predict(Red.X_val_scaled) ##model predictions on validation data\n",
    "rf_mseRed = mean_squared_error(Red.y_val, RfpredictionsRed) ##compute mean square error\\\n",
    "print(\"Random Forest Validation MSE:\",rf_mseRed)\n",
    "##for white same old same old\n",
    "rfWhite = ModelWhite.randomForest()\n",
    "RfpredictionsWhite = rfWhite.predict(White.X_val_scaled) ##model predictions on validation data\n",
    "rf_mseWhite = mean_squared_error(White.y_val, RfpredictionsWhite) ##compute mean square error\n",
    "\n",
    "## Validate SVR model\n",
    "svrRed = ModelRed.SVR() ##created model\n",
    "svr_predictionsRed = svrRed.predict(Red.X_val_scaled) ##made predictions\n",
    "svr_mseRed = mean_squared_error(Red.y_val, svr_predictionsRed)##calculated error\n",
    "print(\"SVR Validation MSE:\", svr_mseRed)\n",
    "#same for white\n",
    "svrWhite = ModelWhite.SVR()\n",
    "svr_predictionsWhite = svrWhite.predict(White.X_val_scaled)\n",
    "svr_mseWhite = mean_squared_error(White.y_val, svr_predictionsWhite)\n",
    "print(\"SVR Validation MSE:\", svr_mseWhite)\n",
    "\n",
    "# Validate Neural Network model\n",
    "nn_modelRed = ModelRed.NN() ##initialized model\n",
    "nn_predictionsRed = nn_modelRed.predict(Red.X_val_scaled) ##predict\n",
    "nn_mseRed = mean_squared_error(Red.y_val, nn_predictionsRed)## cal error\n",
    "print(\"Neural Network Validation MSE:\", nn_mseRed)\n",
    "##same for white\n",
    "nn_modelWhite = ModelWhite.NN() ##initialized model\n",
    "nn_predictionsWhite = nn_modelWhite.predict(White.X_val_scaled) ##predict\n",
    "nn_mseWhite = mean_squared_error(White.y_val, nn_predictionsRed)## cal error\n",
    "print(\"Neural Network Validation MSE:\", nn_mseWhite)\n",
    "\n",
    "# Validate Neural Network model\n",
    "nn_modelRed = ModelRed.NN() ##initialized model\n",
    "nn_predictionsRed = nn_modelRed.predict(Red.X_val_scaled) ##predict\n",
    "nn_mseRed = mean_squared_error(Red.y_val, nn_predictionsRed)## cal error\n",
    "print(\"Neural Network Validation MSE:\", nn_mseRed)\n",
    "##same for white\n",
    "nn_modelWhite = ModelWhite.NN() ##initialized model\n",
    "nn_predictionsWhite = nn_modelWhite.predict(White.X_val_scaled) ##predict\n",
    "nn_mseWhite = mean_squared_error(White.y_val, nn_predictionsWhite)## cal error\n",
    "print(\"Neural Network Validation MSE:\", nn_mseWhite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8147/214265759.py:52: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  nn_model = KerasRegressor(build_fn=self.CreateNeuralNetwork, epochs=50, batch_size=32, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Validation MSE: 0.8898504805946515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8147/214265759.py:52: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  nn_model = KerasRegressor(build_fn=self.CreateNeuralNetwork, epochs=50, batch_size=32, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Validation MSE: 0.42704491259493194\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 1c) importance of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##q1 d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
